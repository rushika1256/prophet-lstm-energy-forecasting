{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":13545611,"datasetId":8602599,"databundleVersionId":14271435,"isSourceIdPinned":false}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-11T17:59:39.907166Z","iopub.execute_input":"2026-01-11T17:59:39.907623Z","iopub.status.idle":"2026-01-11T17:59:40.157014Z","shell.execute_reply.started":"2026-01-11T17:59:39.907600Z","shell.execute_reply":"2026-01-11T17:59:40.156147Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"greninja2006/boujdour\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T17:59:50.007583Z","iopub.execute_input":"2026-01-11T17:59:50.008374Z","iopub.status.idle":"2026-01-11T17:59:54.199799Z","shell.execute_reply.started":"2026-01-11T17:59:50.008350Z","shell.execute_reply":"2026-01-11T17:59:54.199072Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/boujdour\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('/kaggle/input/boujdour/Boujdour 10T.csv', sep=\";\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T17:59:56.944664Z","iopub.execute_input":"2026-01-11T17:59:56.945376Z","iopub.status.idle":"2026-01-11T17:59:57.072329Z","shell.execute_reply.started":"2026-01-11T17:59:56.945349Z","shell.execute_reply":"2026-01-11T17:59:57.071555Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"           DateTime  zone1  zone2  zone3\n0  14/09/2022 17:10  59,81  14,88  60,77\n1  14/09/2022 17:20  59,68  15,08  60,52\n2  14/09/2022 17:30  60,45  15,25  60,63\n3  14/09/2022 17:40  59,72  15,15  59,29\n4  14/09/2022 17:50  60,75  15,60  60,43","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DateTime</th>\n      <th>zone1</th>\n      <th>zone2</th>\n      <th>zone3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14/09/2022 17:10</td>\n      <td>59,81</td>\n      <td>14,88</td>\n      <td>60,77</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>14/09/2022 17:20</td>\n      <td>59,68</td>\n      <td>15,08</td>\n      <td>60,52</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>14/09/2022 17:30</td>\n      <td>60,45</td>\n      <td>15,25</td>\n      <td>60,63</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14/09/2022 17:40</td>\n      <td>59,72</td>\n      <td>15,15</td>\n      <td>59,29</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>14/09/2022 17:50</td>\n      <td>60,75</td>\n      <td>15,60</td>\n      <td>60,43</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"for col in df.columns[1:]:\n  df[col]=df[col].str.replace(\",\",\".\",regex=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T18:00:03.367062Z","iopub.execute_input":"2026-01-11T18:00:03.367553Z","iopub.status.idle":"2026-01-11T18:00:03.441057Z","shell.execute_reply.started":"2026-01-11T18:00:03.367532Z","shell.execute_reply":"2026-01-11T18:00:03.440336Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"for col in df.columns[1:]:\n  df[col]=df[col].astype(float)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T18:00:10.936878Z","iopub.execute_input":"2026-01-11T18:00:10.937156Z","iopub.status.idle":"2026-01-11T18:00:10.973502Z","shell.execute_reply.started":"2026-01-11T18:00:10.937136Z","shell.execute_reply":"2026-01-11T18:00:10.972946Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Ensure 'DateTime' is datetime type\ndf['DateTime'] = pd.to_datetime(df['DateTime'], dayfirst=True, errors='coerce')\n\n# Set DateTime as index\ndf = df.set_index('DateTime')\n\n# Sort by datetime just in case\ndf = df.sort_index()\n\n# Now resampling works\ndata_hourly = df.resample('1h').sum()\ndata_hourly_mean = df.resample('1h').mean()\ndata_daily_mean = data_hourly_mean.resample('1D').mean()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T18:00:17.248339Z","iopub.execute_input":"2026-01-11T18:00:17.248601Z","iopub.status.idle":"2026-01-11T18:00:17.496729Z","shell.execute_reply.started":"2026-01-11T18:00:17.248582Z","shell.execute_reply":"2026-01-11T18:00:17.496180Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!pip install optuna","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T18:00:23.726501Z","iopub.execute_input":"2026-01-11T18:00:23.726765Z","iopub.status.idle":"2026-01-11T18:00:27.864105Z","shell.execute_reply.started":"2026-01-11T18:00:23.726745Z","shell.execute_reply":"2026-01-11T18:00:27.863190Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.5.0)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.17.1)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.10.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.3)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\nRequirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2.4.1)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->optuna) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->optuna) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->optuna) (2024.2.0)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"pip install lightgbm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T18:00:30.732989Z","iopub.execute_input":"2026-01-11T18:00:30.733660Z","iopub.status.idle":"2026-01-11T18:00:33.846207Z","shell.execute_reply.started":"2026-01-11T18:00:30.733631Z","shell.execute_reply":"2026-01-11T18:00:33.845262Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.6.0)\nRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.15.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->lightgbm) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->lightgbm) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->lightgbm) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->lightgbm) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->lightgbm) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->lightgbm) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.0->lightgbm) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.0->lightgbm) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.0->lightgbm) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.0->lightgbm) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.0->lightgbm) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.0->lightgbm) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!pip install tensorflow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T18:00:39.452371Z","iopub.execute_input":"2026-01-11T18:00:39.453008Z","iopub.status.idle":"2026-01-11T18:00:43.655582Z","shell.execute_reply.started":"2026-01-11T18:00:39.452975Z","shell.execute_reply":"2026-01-11T18:00:43.654878Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\nCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.15.0)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\nRequirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\nRequirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.2.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\nDownloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 6.33.0\n    Uninstalling protobuf-6.33.0:\n      Successfully uninstalled protobuf-6.33.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.5 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-5.29.5\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!pip install prophet lightgbm optuna tensorflow -q\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T18:00:48.119057Z","iopub.execute_input":"2026-01-11T18:00:48.119611Z","iopub.status.idle":"2026-01-11T18:00:51.448770Z","shell.execute_reply.started":"2026-01-11T18:00:48.119584Z","shell.execute_reply":"2026-01-11T18:00:51.447806Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ================================================================\n# üìò Robust Hybrid Feature Engineering Pipeline\n# Prophet (Daily) + LSTM (Hourly) + Symbolic & Programmatic Features\n# ================================================================\n\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\n# ================================================================\n# 1Ô∏è‚É£ LOAD AND PREPARE DATA\n# ================================================================\n# Assume you already have:\n#  üîπ data ‚Üí original 10-min resolution dataframe\n#  üîπ data_hourly_mean ‚Üí hourly mean dataframe (aggregated from data)\n# Example: data_hourly_mean = data.resample('H').mean()\n\ndf_hourly = data_hourly_mean.copy()\ndf_hourly.index.name = \"DateTime\"\n\nprint(f\"Raw hourly data shape: {df_hourly.shape}\")\nprint(df_hourly.head())\n\n# ================================================================\n# 2Ô∏è‚É£ FEATURE ENGINEERING UTILITIES\n# ================================================================\n\ndef add_time_features(df):\n    \"\"\"Add calendar and cyclical time features.\"\"\"\n    df = df.copy()\n    df[\"hour\"] = df.index.hour\n    df[\"dayofweek\"] = df.index.dayofweek\n    df[\"month\"] = df.index.month\n    df[\"is_weekend\"] = df[\"dayofweek\"].isin([5,6]).astype(int)\n    df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24)\n    df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24)\n    return df\n\ndef add_lag_and_rolling(df, zones, lags=[1,3,6,12,24], rolls=[3,6,12,24]):\n    \"\"\"Add lag and rolling statistical features.\"\"\"\n    df = df.copy()\n    for z in zones:\n        for l in lags:\n            df[f\"{z}_lag_{l}\"] = df[z].shift(l)\n        for w in rolls:\n            df[f\"{z}_roll_mean_{w}\"] = df[z].rolling(window=w, min_periods=1).mean()\n            df[f\"{z}_roll_std_{w}\"] = df[z].rolling(window=w, min_periods=1).std().fillna(0)\n    return df\n\ndef add_derivatives(df, zones):\n    \"\"\"Add first/second derivatives and percentage change.\"\"\"\n    df = df.copy()\n    for z in zones:\n        df[f\"{z}_diff_1\"] = df[z].diff(1)\n        df[f\"{z}_diff_2\"] = df[z].diff(2)\n        df[f\"{z}_pct_change_1\"] = df[z].pct_change(1).replace([np.inf,-np.inf], np.nan).fillna(0)\n    return df\n\ndef add_fourier_terms(df, period_hours=24, K=3):\n    \"\"\"Add Fourier seasonal terms.\"\"\"\n    df = df.copy()\n    t = np.arange(len(df))\n    for k in range(1, K+1):\n        df[f\"fourier_sin_{k}\"] = np.sin(2*np.pi*k*t/period_hours)\n        df[f\"fourier_cos_{k}\"] = np.cos(2*np.pi*k*t/period_hours)\n    return df\n\ndef add_symbolic_like_features(df, zones):\n    \"\"\"Add interpretable symbolic-like nonlinear feature combinations.\"\"\"\n    df = df.copy()\n    for z in zones:\n        df[f\"{z}_sym_sinlag3_logroll6\"] = np.sin(df[f\"{z}_lag_3\"].fillna(0)) * np.log1p(df[f\"{z}_roll_mean_6\"].fillna(0))\n        df[f\"{z}_sym_lag1_over_lag24\"] = df[f\"{z}_lag_1\"] / (df[f\"{z}_lag_24\"].replace(0, np.nan))\n        df[f\"{z}_sym_prod_diff1_diff2\"] = df[f\"{z}_diff_1\"].fillna(0) * df[f\"{z}_diff_2\"].fillna(0)\n    return df\n\n# ================================================================\n# 3Ô∏è‚É£ APPLY PROGRAMMATIC + SYMBOLIC FEATURE ENGINEERING\n# ================================================================\nzones = [c for c in df_hourly.columns if c.startswith(\"zone\")]\ndf = df_hourly.copy()\ndf = add_time_features(df)\ndf = add_lag_and_rolling(df, zones)\ndf = add_derivatives(df, zones)\ndf = add_fourier_terms(df, period_hours=24, K=2)\ndf = add_symbolic_like_features(df, zones)\n\nprint(f\"‚úÖ After feature engineering: {df.shape[1]} columns\")\n\n# ================================================================\n# 4Ô∏è‚É£ PROPHET-DERIVED DAILY FEATURES (TREND + WEEKLY + YEARLY)\n# ================================================================\ntry:\n    from prophet import Prophet\n    prophet_available = True\nexcept:\n    try:\n        from fbprophet import Prophet\n        prophet_available = True\n    except:\n        prophet_available = False\n\nif prophet_available:\n    print(\"üß≠ Prophet detected ‚Äî extracting daily components...\")\n    daily = df_hourly.sum(axis=1).resample(\"D\").mean().reset_index()\n    daily.columns = [\"ds\", \"y\"]\n\n    m = Prophet(daily_seasonality=False, weekly_seasonality=True, yearly_seasonality=True)\n    m.fit(daily)\n    forecast = m.predict(m.make_future_dataframe(periods=0, freq=\"D\"))\n    comp = forecast[[\"ds\", \"trend\", \"weekly\", \"yearly\", \"yhat\"]].set_index(\"ds\")\n    comp[\"residual\"] = daily.set_index(\"ds\")[\"y\"] - comp[\"yhat\"]\n\n    # Upsample to hourly and align with df\n    comp_hourly = comp.reindex(pd.date_range(comp.index.min(), comp.index.max(), freq=\"H\")).ffill()\n    comp_hourly = comp_hourly.reindex(df.index, method=\"ffill\")\n    for col in comp_hourly.columns:\n        df[f\"prophet_{col}\"] = comp_hourly[col].values\nelse:\n    print(\"‚öôÔ∏è Prophet not available ‚Äî using STL decomposition fallback.\")\n    from statsmodels.tsa.seasonal import STL\n    daily = df_hourly.sum(axis=1).resample(\"D\").mean()\n    stl = STL(daily.interpolate(), period=7)\n    res = stl.fit()\n    comp = pd.DataFrame({\n        \"trend\": res.trend,\n        \"seasonal\": res.seasonal,\n        \"resid\": res.resid\n    })\n    comp_hourly = comp.reindex(pd.date_range(comp.index.min(), comp.index.max(), freq=\"H\")).ffill()\n    comp_hourly = comp_hourly.reindex(df.index, method=\"ffill\")\n    df[\"prophet_trend\"] = comp_hourly[\"trend\"].values\n    df[\"prophet_weekly\"] = comp_hourly[\"seasonal\"].values\n    df[\"prophet_residual\"] = comp_hourly[\"resid\"].values\n\n# ================================================================\n# 5Ô∏è‚É£ LSTM-DERIVED TEMPORAL EMBEDDINGS (OPTIONAL)\n# ================================================================\ntry:\n    import tensorflow as tf\n    from tensorflow.keras.models import Model\n    from tensorflow.keras.layers import LSTM, Dense, Input\n    from tensorflow.keras.callbacks import EarlyStopping\n    tf_available = True\nexcept:\n    tf_available = False\n\nif tf_available:\n    print(\"üî∂ TensorFlow available ‚Äî training LSTM encoder...\")\n    feature_cols = [c for c in df.columns if not c.startswith(\"zone\")] + [f\"{z}_lag_1\" for z in zones]\n    feature_cols = [c for c in feature_cols if c in df.columns]\n    df_train = df[feature_cols].fillna(method=\"ffill\").fillna(0)\n    seq_len = 24  # one-day lookback window\n\n    X, y = [], []\n    total = df_hourly.sum(axis=1)\n    for i in range(len(df_train)-seq_len):\n        X.append(df_train.iloc[i:i+seq_len].values)\n        y.append(total.iloc[i+seq_len])\n    X, y = np.array(X), np.array(y)\n\n    if len(X) > 0:\n        inp = Input(shape=(X.shape[1], X.shape[2]))\n        lstm_layer = LSTM(32, return_sequences=False, name=\"encoder_lstm\")(inp)\n        out = Dense(1, activation=\"linear\")(lstm_layer)\n        model = Model(inputs=inp, outputs=out)\n        model.compile(optimizer=\"adam\", loss=\"mse\")\n        es = EarlyStopping(monitor=\"loss\", patience=5, restore_best_weights=True)\n        model.fit(X, y, epochs=30, batch_size=32, callbacks=[es], verbose=0)\n\n        encoder = Model(inputs=inp, outputs=model.get_layer(\"encoder_lstm\").output)\n        embeddings = encoder.predict(X, verbose=0)\n        emb_df = pd.DataFrame(embeddings, index=df.index[seq_len:seq_len+len(embeddings)])\n        for i_col in range(emb_df.shape[1]):\n            df[f\"lstm_emb_{i_col}\"] = np.nan\n            df.loc[emb_df.index, f\"lstm_emb_{i_col}\"] = emb_df.iloc[:, i_col].values\n    else:\n        print(\"Not enough samples for LSTM embedding.\")\nelse:\n    print(\"‚ùå TensorFlow not available ‚Äî skipping LSTM embedding features.\")\n\n# ================================================================\n# 6Ô∏è‚É£ SAVE & DISPLAY FINAL FEATURE DATASET\n# ================================================================\nprint(f\"\\n‚úÖ Final engineered DataFrame shape: {df.shape}\")\nprint(f\"‚úÖ Total columns: {len(df.columns)}\")\nprint(df.head())\n\ndf.to_csv(\"final_engineered_df.csv\")\nprint(\"üíæ Saved as final_engineered_df.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T18:00:54.530402Z","iopub.execute_input":"2026-01-11T18:00:54.531142Z","iopub.status.idle":"2026-01-11T18:02:15.000953Z","shell.execute_reply.started":"2026-01-11T18:00:54.531113Z","shell.execute_reply":"2026-01-11T18:02:15.000129Z"}},"outputs":[{"name":"stdout","text":"Raw hourly data shape: (14816, 3)\n                         zone1      zone2      zone3\nDateTime                                            \n2022-09-14 17:00:00  60.082000  15.192000  60.328000\n2022-09-14 18:00:00  64.758333  16.280000  58.718333\n2022-09-14 19:00:00  66.251667  17.761667  54.316667\n2022-09-14 20:00:00  79.946667  24.691667  64.728333\n2022-09-14 21:00:00  86.553333  25.910000  70.788333\n‚úÖ After feature engineering: 70 columns\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/2474188632.py:56: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n  df[f\"{z}_pct_change_1\"] = df[z].pct_change(1).replace([np.inf,-np.inf], np.nan).fillna(0)\n/tmp/ipykernel_47/2474188632.py:56: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n  df[f\"{z}_pct_change_1\"] = df[z].pct_change(1).replace([np.inf,-np.inf], np.nan).fillna(0)\n/tmp/ipykernel_47/2474188632.py:56: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n  df[f\"{z}_pct_change_1\"] = df[z].pct_change(1).replace([np.inf,-np.inf], np.nan).fillna(0)\n","output_type":"stream"},{"name":"stdout","text":"üß≠ Prophet detected ‚Äî extracting daily components...\n","output_type":"stream"},{"name":"stderr","text":"18:00:55 - cmdstanpy - INFO - Chain [1] start processing\n18:00:55 - cmdstanpy - INFO - Chain [1] done processing\n/tmp/ipykernel_47/2474188632.py:115: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n  comp_hourly = comp.reindex(pd.date_range(comp.index.min(), comp.index.max(), freq=\"H\")).ffill()\n2026-01-11 18:00:57.052400: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768154457.226625      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768154457.276763      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"üî∂ TensorFlow available ‚Äî training LSTM encoder...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/2474188632.py:152: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  df_train = df[feature_cols].fillna(method=\"ffill\").fillna(0)\nI0000 00:00:1768154472.473290      47 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\nI0000 00:00:1768154475.567676     175 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Final engineered DataFrame shape: (14816, 107)\n‚úÖ Total columns: 107\n                         zone1      zone2      zone3  hour  dayofweek  month  \\\nDateTime                                                                       \n2022-09-14 17:00:00  60.082000  15.192000  60.328000    17          2      9   \n2022-09-14 18:00:00  64.758333  16.280000  58.718333    18          2      9   \n2022-09-14 19:00:00  66.251667  17.761667  54.316667    19          2      9   \n2022-09-14 20:00:00  79.946667  24.691667  64.728333    20          2      9   \n2022-09-14 21:00:00  86.553333  25.910000  70.788333    21          2      9   \n\n                     is_weekend  hour_sin      hour_cos  zone1_lag_1  ...  \\\nDateTime                                                              ...   \n2022-09-14 17:00:00           0 -0.965926 -2.588190e-01          NaN  ...   \n2022-09-14 18:00:00           0 -1.000000 -1.836970e-16    60.082000  ...   \n2022-09-14 19:00:00           0 -0.965926  2.588190e-01    64.758333  ...   \n2022-09-14 20:00:00           0 -0.866025  5.000000e-01    66.251667  ...   \n2022-09-14 21:00:00           0 -0.707107  7.071068e-01    79.946667  ...   \n\n                     lstm_emb_22  lstm_emb_23  lstm_emb_24  lstm_emb_25  \\\nDateTime                                                                  \n2022-09-14 17:00:00          NaN          NaN          NaN          NaN   \n2022-09-14 18:00:00          NaN          NaN          NaN          NaN   \n2022-09-14 19:00:00          NaN          NaN          NaN          NaN   \n2022-09-14 20:00:00          NaN          NaN          NaN          NaN   \n2022-09-14 21:00:00          NaN          NaN          NaN          NaN   \n\n                     lstm_emb_26  lstm_emb_27  lstm_emb_28  lstm_emb_29  \\\nDateTime                                                                  \n2022-09-14 17:00:00          NaN          NaN          NaN          NaN   \n2022-09-14 18:00:00          NaN          NaN          NaN          NaN   \n2022-09-14 19:00:00          NaN          NaN          NaN          NaN   \n2022-09-14 20:00:00          NaN          NaN          NaN          NaN   \n2022-09-14 21:00:00          NaN          NaN          NaN          NaN   \n\n                     lstm_emb_30  lstm_emb_31  \nDateTime                                       \n2022-09-14 17:00:00          NaN          NaN  \n2022-09-14 18:00:00          NaN          NaN  \n2022-09-14 19:00:00          NaN          NaN  \n2022-09-14 20:00:00          NaN          NaN  \n2022-09-14 21:00:00          NaN          NaN  \n\n[5 rows x 107 columns]\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"name":"stdout","text":"üíæ Saved as final_engineered_df.csv\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Robust Hybrid: Prophet + LSTM Weighted Ensemble (fixed sensitivity + improvements)\nimport os, random, time, warnings\nimport numpy as np\nimport pandas as pd\nfrom prophet import Prophet\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport tensorflow as tf\nfrom scipy.optimize import minimize_scalar\nimport shap\nfrom scipy.stats import entropy\n\n# -------------------- Reproducibility --------------------\nos.environ['PYTHONHASHSEED'] = '42'\nrandom.seed(42)\nnp.random.seed(42)\ntf.random.set_seed(42)\ntf.get_logger().setLevel('ERROR')\n# If your TF supports mixed precision and you want it, keep it. Otherwise comment out.\n# tf.keras.mixed_precision.set_global_policy('mixed_float16')\n\nwarnings.filterwarnings(\"ignore\")\n\n# -------------------- Data (user should provide `data_hourly_mean`) --------------------\n# df = data_hourly_mean.copy()  # <- expected provided by user\n# Example: ensure df index is datetime and columns are zones\n# df.index = pd.to_datetime(df.index)\n# for col in df.columns: df[col] = df[col].interpolate().bfill().ffill().fillna(df[col].mean())\n\n# -------------------- Parameters --------------------\ntrain_ratio = 0.8\nn_lags = 48\nepochs = 100\nbatch_size = 32\npatience = 15\nalpha_cpi = 0.05  # 95% conformal interval\ntscv_splits = 3   # (unused in this script, kept for later CV extension)\ntarget_scale_for_lstm = False  # If True: scale target y_train with StandardScaler (can help numeric stability)\n\n# -------------------- Helpers --------------------\ndef train_prophet(series, train_idx):\n    \"\"\"\n    Fit Prophet on daily-aggregated training portion and predict daily yhat for whole series range.\n    Then resample to hourly and align to series index by interpolation.\n    \"\"\"\n    train_series = series.iloc[:train_idx]\n    daily = train_series.resample('D').mean().reset_index()\n    daily.columns = ['ds', 'y']\n    daily['y'] = daily['y'].fillna(method='ffill').fillna(method='bfill').fillna(daily['y'].mean())\n\n    m = Prophet(\n        daily_seasonality=True,\n        weekly_seasonality=True,\n        yearly_seasonality=False,\n        seasonality_mode='multiplicative',\n        changepoint_prior_scale=0.9\n    )\n    m.fit(daily)\n\n    # Forecast from train end date to end of series (so we have predictions over validation/test)\n    start = daily['ds'].min()\n    end = series.index.max().normalize()  # include until max index date\n    fut = pd.DataFrame({'ds': pd.date_range(start, end, freq='D')})\n    forecast = m.predict(fut)\n    # use yhat, then resample to hourly and interpolate to match series index\n    hourly = forecast.set_index('ds')['yhat'].resample('H').interpolate()\n    # reindex to exact series index with interpolation\n    hourly = hourly.reindex(pd.DatetimeIndex(series.index.union(hourly.index))).interpolate().reindex(series.index)\n    return hourly\n\ndef create_lag_features(series, n_lags):\n    feat = pd.DataFrame(index=series.index)\n    for lag in range(1, n_lags + 1):\n        feat[f'lag_{lag}'] = series.shift(lag)\n    # cyclical hour encoding\n    feat['hour_sin'] = np.sin(2*np.pi*series.index.hour/24)\n    feat['hour_cos'] = np.cos(2*np.pi*series.index.hour/24)\n    feat['dow'] = series.index.dayofweek\n    feat['month'] = series.index.month\n    feat['is_weekend'] = (series.index.weekday >= 5).astype(int)\n    feat['trend_24h'] = series.rolling(24, min_periods=1).mean()\n    feat['roll_mean_12'] = series.rolling(12, min_periods=1).mean()\n    feat['roll_std_12'] = series.rolling(12, min_periods=1).std().fillna(0)\n    return feat\n\ndef build_lstm(input_shape):\n    model = Sequential([\n        LSTM(256, return_sequences=True, input_shape=input_shape),\n        Dropout(0.2),\n        LSTM(128),\n        Dense(64, activation='relu'),\n        Dropout(0.1),\n        Dense(32, activation='relu'),\n        Dense(1, dtype='float32')\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(3e-4), loss='mse')\n    return model\n\ndef make_predict_fn(model):\n    def predict_fn(X_2d):\n        X = np.array(X_2d, dtype=np.float32)\n        if X.ndim == 1:\n            X = X.reshape(1, -1)\n        # expected shape: (n_samples, n_features) -> reshape to (n_samples, n_features, 1)\n        return model.predict(X.reshape((X.shape[0], X.shape[1], 1)), verbose=0).flatten()\n    return predict_fn\n\ndef compute_residual_entropy(residuals, bins=50):\n    hist, _ = np.histogram(residuals, bins=bins, density=True)\n    hist = hist + 1e-12  # avoid log(0)\n    return entropy(hist)\n\ndef compute_cpi(y_true, y_pred, alpha=0.05):\n    # compute (1-alpha) quantile of absolute residuals\n    abs_res = np.abs(y_true - y_pred)\n    q = np.quantile(abs_res, 1 - alpha)\n    lower = y_pred - q\n    upper = y_pred + q\n    coverage = np.mean((y_true >= lower) & (y_true <= upper))\n    width = np.mean(upper - lower)\n    return coverage, width, q\n\n# -------------------- Main Loop --------------------\ndef run_hybrid(df):\n    zones = df.columns.tolist()\n    print(\"Zones:\", zones, \" NaNs after cleaning:\", df.isna().sum().sum())\n    results = []\n    start_time = time.time()\n\n    for zone in zones:\n        print(f\"\\n--- Zone: {zone} ---\")\n        series = df[zone].astype(float)\n        n = len(series)\n        split_idx = int(n * train_ratio)\n\n        # Prophet: train on train portion and predict for full period\n        prophet_pred_series = train_prophet(series, split_idx)\n\n        # LSTM features\n        feat = create_lag_features(series, n_lags)\n        feat['y_true'] = series\n        supervised = feat.dropna()\n        # ensure we still have enough samples\n        if supervised.shape[0] < 100:\n            print(\"Warning: too few supervised samples after lagging for zone\", zone)\n        train_mask = supervised.index < series.index[split_idx]\n        train_df, val_df = supervised.loc[train_mask], supervised.loc[~train_mask]\n\n        X_train = train_df.drop(columns=['y_true']).values.astype(np.float32)\n        y_train = train_df['y_true'].values.astype(np.float32)\n        X_val = val_df.drop(columns=['y_true']).values.astype(np.float32)\n        y_val = val_df['y_true'].values.astype(np.float32)\n\n        # scale features\n        scaler = StandardScaler()\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_val_scaled = scaler.transform(X_val)\n\n        # optional: scale target for better numeric stability in LSTM\n        if target_scale_for_lstm:\n            y_scaler = StandardScaler().fit(y_train.reshape(-1,1))\n            y_train_scaled = y_scaler.transform(y_train.reshape(-1,1)).reshape(-1)\n            # note: at inference, remember to inverse transform predictions\n        else:\n            y_train_scaled = y_train.copy()\n\n        X_train_3d = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n        X_val_3d = X_val_scaled.reshape((X_val_scaled.shape[0], X_val_scaled.shape[1], 1))\n\n        # LSTM model training\n        lstm_model = build_lstm((X_train_3d.shape[1], 1))\n        es = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True, verbose=0)\n        lstm_model.fit(X_train_3d, y_train_scaled, validation_data=(X_val_3d, y_val),\n                       epochs=epochs, batch_size=batch_size, verbose=0, callbacks=[es])\n\n        # Predictions from LSTM\n        lstm_val_pred = lstm_model.predict(X_val_3d, verbose=0).flatten()\n        if target_scale_for_lstm:\n            # inverse transform\n            lstm_val_pred = y_scaler.inverse_transform(lstm_val_pred.reshape(-1,1)).reshape(-1)\n\n        # Prophet val predictions aligned to val_df index\n        prophet_val_pred = prophet_pred_series.loc[val_df.index].values\n\n        # Optimize weight w on validation set\n        def obj_w(w):\n            blended = w * prophet_val_pred + (1.0 - w) * lstm_val_pred\n            return np.sqrt(mean_squared_error(y_val, blended))\n        res = minimize_scalar(obj_w, bounds=(0.0, 1.0), method='bounded')\n        w_opt = float(res.x) if res.success else 0.5\n        hybrid_val_pred = w_opt * prophet_val_pred + (1.0 - w_opt) * lstm_val_pred\n\n        # Conformal Prediction Interval (CPI)\n        cpi_cov, cpi_width, cpi_q = compute_cpi(y_val, hybrid_val_pred, alpha=alpha_cpi)\n\n        # SHAP feature importance (try real SHAP, fallback to permutation importance)\n        shap_series = None\n        try:\n            predict_fn = make_predict_fn(lstm_model)\n            bg_idx = np.random.choice(X_train_scaled.shape[0], min(100, X_train_scaled.shape[0]), replace=False)\n            X_bg = X_train_scaled[bg_idx]\n            explainer = shap.Explainer(predict_fn, X_bg)\n            # limit to first N val rows to speed up\n            shap_vals = explainer(X_val_sample, max_evals=100)\n            shap_series = pd.Series(np.mean(np.abs(shap_vals.values), axis=0),\n                                    index=train_df.drop(columns=['y_true']).columns).sort_values(ascending=False)\n        except Exception as e:\n            # fallback: simple permutation importance (correlation-based)\n            try:\n                cols = train_df.drop(columns=['y_true']).columns\n                perm_imp = {}\n                base_rmse = np.sqrt(mean_squared_error(y_val, lstm_val_pred))\n                for i, col in enumerate(cols):\n                    X_val_perm = X_val_scaled.copy()\n                    np.random.shuffle(X_val_perm[:, i])\n                    pred_perm = lstm_model.predict(X_val_perm.reshape((X_val_perm.shape[0], X_val_perm.shape[1], 1)), verbose=0).flatten()\n                    if target_scale_for_lstm:\n                        pred_perm = y_scaler.inverse_transform(pred_perm.reshape(-1,1)).reshape(-1)\n                    rmse_perm = np.sqrt(mean_squared_error(y_val, pred_perm))\n                    perm_imp[col] = rmse_perm - base_rmse\n                shap_series = pd.Series(perm_imp).sort_values(ascending=False)\n            except Exception:\n                shap_series = pd.Series(np.abs(pd.DataFrame(X_train_scaled, columns=train_df.drop(columns=['y_true']).columns).corrwith(pd.Series(y_train))).sort_values(ascending=False))\n\n        # Residual entropy\n        res_entropy = compute_residual_entropy(y_val - hybrid_val_pred)\n\n        # ------------ Fixed Perturbation Sensitivity ------------\n        # Proper approach: perturb the validation feature matrix and compute output changes.\n        eps = 1e-8\n        # generate noise scaled per-feature (10% of feature std); tune scale factor if sensitivity too large\n        per_feature_std = np.std(X_val_scaled, axis=0, ddof=1)\n        # If a feature std is zero, use small value to avoid zero noise\n        per_feature_std[per_feature_std == 0] = 1e-6\n        noise_scale = 0.01  # 1% noise; you can reduce to 0.001 if this is still large\n        noise = np.random.normal(0, noise_scale * per_feature_std, X_val_scaled.shape).astype(np.float32)\n        # Perturb the actual validation features\n        X_val_perturbed = X_val_scaled + noise\n        # predict with perturbed inputs\n        y_perturbed = lstm_model.predict(X_val_perturbed.reshape((X_val_perturbed.shape[0], X_val_perturbed.shape[1], 1)), verbose=0).flatten()\n        if target_scale_for_lstm:\n            y_perturbed = y_scaler.inverse_transform(y_perturbed.reshape(-1,1)).reshape(-1)\n        # relative change metric, averaged\n        sensitivity = np.mean(np.abs(y_perturbed - lstm_val_pred) / (np.abs(lstm_val_pred) + eps))\n\n        # Metrics\n        rmse_prophet = np.sqrt(mean_squared_error(y_val, prophet_val_pred))\n        r2_prophet = r2_score(y_val, prophet_val_pred)\n        rmse_lstm = np.sqrt(mean_squared_error(y_val, lstm_val_pred))\n        r2_lstm = r2_score(y_val, lstm_val_pred)\n        rmse_final = np.sqrt(mean_squared_error(y_val, hybrid_val_pred))\n        r2_final = r2_score(y_val, hybrid_val_pred)\n\n        print(f\"w={w_opt:.3f} | Prophet R2={r2_prophet:.3f}, LSTM R2={r2_lstm:.3f} | Hybrid R2={r2_final:.3f}\")\n        print(f\"CPI cov={cpi_cov:.3f}, width={cpi_width:.3f}, q={cpi_q:.4f} | Sensitivity={sensitivity:.4f} | Residual entropy={res_entropy:.4f}\")\n\n        results.append({\n            'zone': zone,\n            'w': w_opt,\n            'rmse_prophet': rmse_prophet, 'r2_prophet': r2_prophet,\n            'rmse_lstm': rmse_lstm, 'r2_lstm': r2_lstm,\n            'rmse_final': rmse_final, 'r2_final': r2_final,\n            'cpi_cov': cpi_cov, 'cpi_width': cpi_width,\n            'cpi_q': cpi_q,\n            'res_entropy': res_entropy,\n            'sensitivity': sensitivity,\n            'shap_series': shap_series\n        })\n\n    res_df = pd.DataFrame(results)\n    print(\"\\n=== Summary Across Zones ===\")\n    display_cols = ['zone','w','rmse_prophet','r2_prophet','rmse_lstm','r2_lstm','rmse_final','r2_final','cpi_cov','cpi_width','res_entropy','sensitivity']\n    print(res_df[display_cols].round(4).to_string(index=False))\n    print(f\"\\nTotal runtime: {time.time()-start_time:.1f}s\")\n    return res_df\n\n# -------------------- Usage --------------------\n# Ensure `data_hourly_mean` is loaded and accessible as a DataFrame\n# Example: df = data_hourly_mean.copy(); df.index = pd.to_datetime(df.index)\n# res_df = run_hybrid(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}